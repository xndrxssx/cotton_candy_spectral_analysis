{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtenção dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Definindo o caminho base para a pasta Pre-processamento\n",
    "base_path = Path(parent_dir) / 'Partial Components Analysis'\n",
    "\n",
    "file_path_raw_cal = base_path / 'raw_calibration_data.xlsx'\n",
    "file_path_msc_cal = base_path / 'msc_calibration_data.xlsx'\n",
    "file_path_snv_cal = base_path / 'snv_calibration_data.xlsx'\n",
    "file_path_sg_cal = base_path / 'sg_calibration_data.xlsx'\n",
    "\n",
    "df_raw_cal = pd.read_excel(file_path_raw_cal)\n",
    "df_msc_cal = pd.read_excel(file_path_msc_cal)\n",
    "df_snv_cal = pd.read_excel(file_path_snv_cal)\n",
    "df_sg_cal = pd.read_excel(file_path_sg_cal)\n",
    "\n",
    "file_path_raw_val = base_path / 'raw_validation_data.xlsx'\n",
    "file_path_msc_val = base_path / 'msc_validation_data.xlsx'\n",
    "file_path_snv_val = base_path / 'snv_validation_data.xlsx'\n",
    "file_path_sg_val = base_path / 'sg_validation_data.xlsx'\n",
    "\n",
    "df_raw_val = pd.read_excel(file_path_raw_val)\n",
    "df_msc_val = pd.read_excel(file_path_msc_val)\n",
    "df_snv_val = pd.read_excel(file_path_snv_val)\n",
    "df_sg_val = pd.read_excel(file_path_sg_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralização dos dados\n",
    "def mean_center_data(X):\n",
    "    return X - np.mean(X, axis=0)\n",
    "\n",
    "# Detecção de outliers usando Hotelling's T2 e F-residual\n",
    "def detect_outliers(pls, X, Y, n_components):\n",
    "    T2_limit = 2 * n_components * (len(X) - 1) / (len(X) - n_components)\n",
    "    Q_limit = 2 * np.mean((Y - pls.predict(X)) ** 2)  # F-residual limit\n",
    "\n",
    "    T2 = np.sum((pls.x_scores_ / np.sqrt(pls.x_scores_.shape[0] - 1))**2, axis=1)\n",
    "    Q = np.mean((Y - pls.predict(X)) ** 2, axis=1)\n",
    "\n",
    "    return T2, Q, T2_limit, Q_limit\n",
    "\n",
    "# Determinação do número ótimo de componentes\n",
    "def determine_optimal_components(X_centered, Y, max_components=15):\n",
    "    mean_explained_variance = []\n",
    "    std_explained_variance = []\n",
    "    \n",
    "    for n in range(1, max_components + 1):\n",
    "        pls = PLSRegression(n_components=n)\n",
    "        Y_pred = cross_val_predict(pls, X_centered, Y, cv=len(X_centered))  # LOO-CV\n",
    "        explained_variance = r2_score(Y, Y_pred)\n",
    "        mean_explained_variance.append(explained_variance)\n",
    "    \n",
    "    mean_explained_variance = np.array(mean_explained_variance)\n",
    "    min_diff_index = np.argmax(np.diff(mean_explained_variance) <= 0.009)\n",
    "    \n",
    "    optimal_components = min_diff_index + 1  # Adicionar 1 para obter o número correto de componentes\n",
    "    return optimal_components, mean_explained_variance\n",
    "\n",
    "# Avaliação do modelo\n",
    "def evaluate_model(pls, X_test, Y_test):\n",
    "    Y_pred = pls.predict(X_test)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "    bias = np.mean(Y_pred - Y_test)\n",
    "    slope = np.polyfit(Y_test.flatten(), Y_pred.flatten(), 1)[0]\n",
    "    offset = np.polyfit(Y_test.flatten(), Y_pred.flatten(), 1)[1]\n",
    "    return r2, rmse, bias, Y_pred, slope, offset\n",
    "\n",
    "# Predição em novos dados\n",
    "def predict_new_data(pls, X_new, Y_reference):\n",
    "    Y_pred = pls.predict(X_new)\n",
    "    diff = Y_pred - Y_reference\n",
    "    results_df = pd.DataFrame({'Predicted': Y_pred.flatten(), 'Reference': Y_reference.flatten(), 'Difference': diff.flatten()})\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "X = df_msc_cal.iloc[:,1:].values  # Exemplo de dados de comprimento de onda (350 a 2500 nm)\n",
    "Y = df_msc_cal.iloc[:,0].values    # Exemplo de atributo de qualidade\n",
    "\n",
    "# Centralizar os dados\n",
    "X_centered = mean_center_data(X)\n",
    "\n",
    "# Determinar o número ótimo de componentes\n",
    "optimal_components, mean_explained_variance = determine_optimal_components(X_centered, Y)\n",
    "print(f\"Número ótimo de componentes: {optimal_components}\")\n",
    "\n",
    "# Treinar o modelo PLSR com o número ótimo de componentes\n",
    "pls = PLSRegression(n_components=optimal_components)\n",
    "pls.fit(X_centered, Y)\n",
    "\n",
    "# Detectar outliers\n",
    "T2, Q, T2_limit, Q_limit = detect_outliers(pls, X_centered, Y, optimal_components)\n",
    "print(f\"T2 Limit: {T2_limit}, Q Limit: {Q_limit}\")\n",
    "\n",
    "# Avaliação do modelo\n",
    "X_test = np.random.rand(20, 2151)  # Exemplo de dados de teste\n",
    "Y_test = np.random.rand(20, 1)     # Exemplo de atributo de qualidade para teste\n",
    "X_test_centered = mean_center_data(X_test)\n",
    "\n",
    "r2, rmse, bias, Y_pred, slope, offset = evaluate_model(pls, X_test_centered, Y_test)\n",
    "print(f\"R2: {r2}, RMSE: {rmse}, Bias: {bias}, Slope: {slope}, Offset: {offset}\")\n",
    "\n",
    "# Plotando os resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_test, Y_pred, color='blue')\n",
    "plt.plot(Y_test, slope * Y_test + offset, color='red')\n",
    "\n",
    "# Adicionando as métricas no gráfico\n",
    "plt.text(0.05, 0.95, f'Slope: {slope:.6f}\\nOffset: {offset:.6f}\\nRMSE: {rmse:.6f}\\nR-Square: {r2:.6f}',\n",
    "         transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white'))\n",
    "\n",
    "plt.xlabel('Reference Y')\n",
    "plt.ylabel('Predicted Y')\n",
    "plt.title('Predicted vs. Reference')\n",
    "plt.show()\n",
    "\n",
    "# Predição em novos dados\n",
    "new_data = np.random.rand(10, 2151)  # Novos dados de comprimento de onda\n",
    "Y_reference = np.random.rand(10, 1)  # Referência de atributo de qualidade\n",
    "results_df = predict_new_data(pls, new_data, Y_reference)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
