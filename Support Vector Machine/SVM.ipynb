{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Definindo o caminho base para a pasta Pre-processamento\n",
    "base_path = Path(parent_dir) / 'Partial Components Analysis'\n",
    "\n",
    "file_path_raw_cal = base_path / 'RAW_calibration.xlsx'\n",
    "file_path_msc_cal = base_path / 'MSC_calibration.xlsx'\n",
    "file_path_snv_cal = base_path / 'SNV_calibration.xlsx'\n",
    "file_path_sg_cal = base_path / 'SG_calibration.xlsx'\n",
    "\n",
    "df_raw_cal = pd.read_excel(file_path_raw_cal)\n",
    "df_msc_cal = pd.read_excel(file_path_msc_cal)\n",
    "df_snv_cal = pd.read_excel(file_path_snv_cal)\n",
    "df_sg_cal = pd.read_excel(file_path_sg_cal)\n",
    "\n",
    "file_path_raw_val = base_path / 'RAW_validation.xlsx'\n",
    "file_path_msc_val = base_path / 'MSC_validation.xlsx'\n",
    "file_path_snv_val = base_path / 'SNV_validation.xlsx'\n",
    "file_path_sg_val = base_path / 'SG_validation.xlsx'\n",
    "\n",
    "df_raw_val = pd.read_excel(file_path_raw_val)\n",
    "df_msc_val = pd.read_excel(file_path_msc_val)\n",
    "df_snv_val = pd.read_excel(file_path_snv_val)\n",
    "df_sg_val = pd.read_excel(file_path_sg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_msc_val.iloc[:,6:], df_msc_val['SST'].values\n",
    "X_train, y_train = df_msc_cal.iloc[:,6:], df_msc_cal['SST'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((175, 2151), (75, 2151))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "#X_train = pd.DataFrame(X_train, columns=[cols])\n",
    "\n",
    "#X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações do DataFrame original:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Columns: 2151 entries, 350 to 2500\n",
      "dtypes: float64(2151)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "        350       351       352       353       354       355       356  \\\n",
      "0  0.170983  0.188909  0.170686  0.195838  0.175164  0.152250  0.181401   \n",
      "1  0.236949  0.233393  0.244321  0.243989  0.251346  0.269510  0.287860   \n",
      "2  0.194089  0.187573  0.165577  0.185321  0.192728  0.190414  0.194685   \n",
      "3  0.162324  0.176561  0.160116  0.151727  0.172748  0.191431  0.175671   \n",
      "4  0.252257  0.249113  0.237139  0.236524  0.238542  0.236793  0.231172   \n",
      "\n",
      "        357       358       359  ...      2491      2492      2493      2494  \\\n",
      "0  0.178692  0.193972  0.213864  ...  0.018903  0.019636  0.021701  0.022928   \n",
      "1  0.267363  0.264446  0.279330  ...  0.017043  0.016028  0.014980  0.014816   \n",
      "2  0.201946  0.201325  0.194473  ...  0.017283  0.017068  0.017813  0.019254   \n",
      "3  0.169466  0.178471  0.190839  ...  0.024089  0.023431  0.022900  0.021878   \n",
      "4  0.243849  0.254627  0.255052  ...  0.019080  0.018689  0.018581  0.019085   \n",
      "\n",
      "       2495      2496      2497      2498      2499      2500  \n",
      "0  0.024231  0.023868  0.022058  0.023197  0.023155  0.021383  \n",
      "1  0.014132  0.014464  0.015780  0.017419  0.017928  0.016917  \n",
      "2  0.019211  0.019246  0.019638  0.019447  0.019658  0.020022  \n",
      "3  0.021035  0.020640  0.021584  0.023066  0.023800  0.025672  \n",
      "4  0.020352  0.020974  0.020914  0.020030  0.019627  0.019411  \n",
      "\n",
      "[5 rows x 2151 columns]\n",
      "Informações do DataFrame escalonado:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Columns: 2151 entries, 350 to 2500\n",
      "dtypes: float64(2151)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "        350       351       352       353       354       355       356  \\\n",
      "0 -1.494315 -1.240538 -1.469667 -1.139983 -1.438305 -1.757459 -1.415020   \n",
      "1 -0.393658 -0.493083 -0.288202 -0.329664 -0.191237  0.122340  0.462433   \n",
      "2 -1.108781 -1.262984 -1.551636 -1.316978 -1.150785 -1.145655 -1.180740   \n",
      "3 -1.638778 -1.448022 -1.639266 -1.882325 -1.477849 -1.129345 -1.516070   \n",
      "4 -0.138245 -0.228931 -0.403436 -0.455294 -0.400839 -0.402156 -0.537277   \n",
      "\n",
      "        357       358       359  ...      2491      2492      2493      2494  \\\n",
      "0 -1.435972 -1.229647 -0.965705  ... -0.770095 -0.499410  0.143555  0.532595   \n",
      "1  0.174345  0.105103  0.349858  ... -1.395091 -1.681718 -2.016540 -2.014686   \n",
      "2 -1.013668 -1.090388 -1.355377  ... -1.314594 -1.340880 -1.106079 -0.621171   \n",
      "3 -1.603523 -1.523231 -1.428404  ...  0.973114  0.744134  0.528970  0.203072   \n",
      "4 -0.252681 -0.080868 -0.138026  ... -0.710473 -0.809657 -0.859072 -0.674087   \n",
      "\n",
      "       2495      2496      2497      2498      2499      2500  \n",
      "0  0.928719  0.850442  0.308743  0.672157  0.643213 -0.009482  \n",
      "1 -2.248605 -2.164304 -1.691007 -1.253975 -1.081582 -1.482896  \n",
      "2 -0.650633 -0.631471 -0.462154 -0.578095 -0.510745 -0.458283  \n",
      "3 -0.076692 -0.184457  0.157835  0.628447  0.856093  1.405790  \n",
      "4 -0.291534 -0.077378 -0.055589 -0.383690 -0.520849 -0.660022  \n",
      "\n",
      "[5 rows x 2151 columns]\n",
      "Após remoção de baixa variância:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Columns: 2151 entries, 350 to 2500\n",
      "dtypes: float64(2151)\n",
      "memory usage: 2.9 MB\n",
      "None\n",
      "Colunas restantes após a limpeza: Index(['350', '351', '352', '353', '354', '355', '356', '357', '358', '359',\n",
      "       ...\n",
      "       '2491', '2492', '2493', '2494', '2495', '2496', '2497', '2498', '2499',\n",
      "       '2500'],\n",
      "      dtype='object', length=2151)\n",
      "X_train_cleaned shape: (175, 2151)\n",
      "Nenhuma coluna restante após a remoção de colunas identificadoras.\n",
      "Erro ao remover colunas com baixa correlação: The truth value of a RangeIndex is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Após remoção de baixa correlação:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Empty DataFrame\n",
      "None\n",
      "Treinando o modelo com dados limpos:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Garantir que os dados estejam limpos e prontos\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinando o modelo com dados limpos:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Fazer previsões no conjunto de treinamento\u001b[39;00m\n\u001b[0;32m    129\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_cleaned)\n",
      "File \u001b[1;32mc:\\Users\\lenau\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenau\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lenau\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\lenau\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     )\n\u001b[1;32m-> 1192\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\lenau\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:833\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    829\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    830\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[0;32m    831\u001b[0m )\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m--> 833\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[0;32m    836\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Função para calcular as métricas\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    correlation_coefficient = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mean_y_true = np.mean(y_true)\n",
    "    relative_absolute_error = 100 * (mae / np.mean(np.abs(y_true - mean_y_true)))\n",
    "    root_relative_squared_error = 100 * (rmse / np.std(y_true))\n",
    "\n",
    "    return {\n",
    "        \"Correlation coefficient\": correlation_coefficient,\n",
    "        \"Mean absolute error\": mae,\n",
    "        \"Root mean squared error\": rmse,\n",
    "        \"Relative absolute error\": relative_absolute_error,\n",
    "        \"Root relative squared error\": root_relative_squared_error,\n",
    "        \"Total Number of Instances\": len(y_true)\n",
    "    }\n",
    "\n",
    "def display_metrics(title, metrics):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Função para remover colunas com baixa variância\n",
    "def remove_low_variance_columns(df, threshold=0.01):\n",
    "    try:\n",
    "        df_numeric = df.select_dtypes(include=[np.number])\n",
    "        if df_numeric.empty:\n",
    "            raise ValueError(\"O DataFrame não contém colunas numéricas.\")\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        selector.fit(df_numeric)\n",
    "        cols_to_keep = selector.get_support(indices=True)\n",
    "        if len(cols_to_keep) == 0:\n",
    "            raise ValueError(\"Todas as colunas foram removidas devido à baixa variância.\")\n",
    "        return df_numeric.iloc[:, cols_to_keep]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao remover colunas com baixa variância: {e}\")\n",
    "        return df\n",
    "\n",
    "# Função para detectar e remover colunas que são identificadores\n",
    "def remove_identifier_columns(df):\n",
    "    try:\n",
    "        cols_to_remove = [col for col in df.columns if df[col].nunique() == len(df)]\n",
    "        if not cols_to_remove:\n",
    "            raise ValueError(\"Nenhuma coluna identificadora foi encontrada.\")\n",
    "        return df.drop(cols_to_remove, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao remover colunas identificadoras: {e}\")\n",
    "        return df\n",
    "\n",
    "# Função para remover colunas com baixa correlação com o alvo\n",
    "def remove_low_correlation_columns(df, target_column, threshold=0.1):\n",
    "    try:\n",
    "        correlations = df.apply(lambda x: pearsonr(x, df[target_column])[0] if x.name != target_column else 1)\n",
    "        cols_to_remove = correlations[correlations.abs() < threshold].index\n",
    "        if not cols_to_remove:\n",
    "            raise ValueError(\"Nenhuma coluna com baixa correlação foi encontrada.\")\n",
    "        return df.drop(cols_to_remove, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao remover colunas com baixa correlação: {e}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"Informações do DataFrame original:\")\n",
    "print(X_train.info())\n",
    "print(X_train.head())\n",
    "\n",
    "# Aplicar o StandardScaler aos dados\n",
    "try:\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao aplicar o StandardScaler: {e}\")\n",
    "\n",
    "# Verificar os dados após o escalonamento\n",
    "print(\"Informações do DataFrame escalonado:\")\n",
    "print(X_train_scaled.info())\n",
    "print(X_train_scaled.head())\n",
    "\n",
    "# Aplicar as funções de limpeza ao dataset escalonado\n",
    "# Remoção de colunas com baixa variância\n",
    "X_train_cleaned = remove_low_variance_columns(X_train_scaled)\n",
    "if X_train_cleaned.shape[1] == 0:\n",
    "    print(\"Nenhuma coluna restante após a remoção de baixa variância.\")\n",
    "else:\n",
    "    print(\"Após remoção de baixa variância:\")\n",
    "    print(X_train_cleaned.info())\n",
    "\n",
    "print(\"Colunas restantes após a limpeza:\", X_train_cleaned.columns)\n",
    "print(\"X_train_cleaned shape:\", X_train_cleaned.shape)\n",
    "\n",
    "\n",
    "# Remoção de colunas identificadoras\n",
    "X_train_cleaned = remove_identifier_columns(X_train_cleaned)\n",
    "if X_train_cleaned.shape[1] == 0:\n",
    "    print(\"Nenhuma coluna restante após a remoção de colunas identificadoras.\")\n",
    "else:\n",
    "    print(\"Após remoção de colunas identificadoras:\")\n",
    "    print(X_train_cleaned.info())\n",
    "\n",
    "X_train_cleaned = remove_low_correlation_columns(X_train_cleaned, 'SST')  # Substitua 'SST' pelo nome da sua coluna alvo\n",
    "print(\"Após remoção de baixa correlação:\")\n",
    "print(X_train_cleaned.info())\n",
    "\n",
    "# Configurar e treinar o modelo SVR\n",
    "model = SVR(\n",
    "    C=5,             # Regularização\n",
    "    epsilon=0.001,     # Tolerância\n",
    "    kernel='poly',     # Kernel polinomial\n",
    "    degree=1,          # Grau do polinômio\n",
    "    coef0=1,           # Coeficiente do polinômio\n",
    "    tol=1.0E-12        # Tolerância de precisão\n",
    ")\n",
    "\n",
    "# Garantir que os dados estejam limpos e prontos\n",
    "print(\"Treinando o modelo com dados limpos:\")\n",
    "model.fit(X_train_cleaned, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de treinamento\n",
    "y_train_pred = model.predict(X_train_cleaned)\n",
    "training_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "display_metrics(\"Training Metrics\", training_metrics)\n",
    "\n",
    "folds = 175  # Defina o número de folds para a validação cruzada\n",
    "y_train_cv = cross_val_predict(model, X_train_cleaned, y_train, cv=folds)\n",
    "cv_metrics = calculate_metrics(y_train, y_train_cv)\n",
    "display_metrics(\"Cross-Validation Metrics\", cv_metrics)\n",
    "\n",
    "y_pred_val = model.predict(X_test)\n",
    "validation_metrics = calculate_metrics(y_test, y_pred_val)\n",
    "display_metrics(\"Validation Metrics\", validation_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 10, 'epsilon': 0.1, 'gamma': 0.01, 'kernel': 'rbf', 'tol': 1e-08}\n",
      "\n",
      "=== Optimized Training Metrics ===\n",
      "Correlation coefficient: 0.9998\n",
      "Mean absolute error: 0.0978\n",
      "Root mean squared error: 0.0986\n",
      "Relative absolute error: 4.4309\n",
      "Root relative squared error: 3.7124\n",
      "Total Number of Instances: 175.0000\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [1, 10, 50],        # Ajustando C para valores menores\n",
    "    'epsilon': [0.1, 0.2, 0.3],  # Aumentando epsilon para permitir maior tolerância\n",
    "    'gamma': [0.01, 0.1],    # Aumentando gamma para reduzir a sensibilidade do modelo\n",
    "    'kernel': ['rbf'],\n",
    "    'tol': [1e-08, 1e-06]    # Aumentando tol para uma otimização menos agressiva\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=SVR(), param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Treinando e avaliando o modelo otimizado\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "training_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "display_metrics(\"Optimized Training Metrics\", training_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
